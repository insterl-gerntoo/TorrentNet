DHT:

Each node has a UID

Each node keeps a list of contacts

Each node tries to get both the nodes closest to its UID, and the nodes furthest from its UID
	The furthest nodes thing is to allow for faster DHT lookups; New nodes trying to establish their contacts
	can make bigger jumps that way.


	
A node fills its DHT table by doing get_nodes on each new node it finds until it can't find any nodes closer to its own.
	Nodes furthest from its own will be filled in gradually as time goes on and it finds new nodes, it isn't neccessary for
		the basic functionality to set this up immediately upon startup.
	//I just thought of something that might work better for lookup speed than keeping a list of nodes furthest away;
		I guess this is pretty much what the chord algoritm does.
		We'd have a list of hashes, with a fixed length, say 31 maybe.  Each hash would be spaced apart by (2^(hash size in bits))/32
			The initial element would then be (our hash)+((2^(hash size in bits))/32), and we'd go from there.
			Each element after that would be (previous hash)+((2^(hash size in bits))/32)
		Then we'd have another list with the nodes whose ID's are closest to ones in that list.
		This will give us 31 somewhat equadistant points in the ID space to start from.
		
Like the bittorrent DHT, the contact list is construced of buckets, starting with an initial bucket that covers the
	entire ID space.
	These buckets should probably be bigger than in bittorrent, because we'll be dealing with much larger
		networks. Bigger buckets would help make faster DHT lookups, but will also mean doing keep-alive
		on more nodes, so don't go overboard.
		
	Anyways, like in bittorent's DHT, the buckets get full. When a new node is found, if our ID falls in the range we split it,
		if not:
			if there are bad nodes in the bucket, replace them with the new node
			else discard the new node
	
	Nodes are hit with ping queries occasionally just to make sure they're still good.  Probably every 30-60 seconds.

	If a node fails to respond to a query, it becomes questionable.
			
	If a node fails to respond to multiple queries, it becomes bad.
			
	We'll also maintain a last updated property too, buckets must be refreshed after 15 minutes of inactivity:
		Pick a random node in the bucket, then do get_nodes() on it
